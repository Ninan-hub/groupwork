{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "import xml.dom.minidom \n",
    "import re\n",
    "import math\n",
    "import scipy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from gensim import models\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils import datapath\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from time import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank搜索\n",
    "\n",
    "这一部分是读取index和xml，输入query（书名、作者名或描述），返回文件名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Search():\n",
    "    \n",
    "    def __init__(self, index_path,title_index_path,writer_index_path ,csv_path):\n",
    "        #读index\n",
    "        #Rank用list\n",
    "        self.word_list = []\n",
    "        self.Document_f = []\n",
    "        self.word_document_number = []\n",
    "        self.word_tf = []\n",
    "        self.word_docu_position = []\n",
    "        self.stop_words = []\n",
    "        #书名用list\n",
    "        self.title_list = []\n",
    "        self.title_document_number = []\n",
    "        self.title_docu_position = []\n",
    "        #作者名用list\n",
    "        self.writer_list = []\n",
    "        self.writer_document_number = []\n",
    "        self.writer_docu_position = []\n",
    "        #读三个index\n",
    "        self.index_path = index_path\n",
    "        self.ReadIndex(index_path)\n",
    "        self.ReadTIndex(title_index_path)\n",
    "        self.ReadWIndex(writer_index_path)\n",
    "        self.ReadCSV(csv_path)\n",
    "        #for LSVM\n",
    "        self.wordDic = {}\n",
    "        self.ReadLSVM_dic()\n",
    "        \n",
    "            \n",
    "    #读index函数\n",
    "    def ReadIndex(self, index_path):\n",
    "        word_list = []\n",
    "        Document_f = []\n",
    "        word_document_number = []\n",
    "        word_tf = []\n",
    "        dn = []\n",
    "        tf = []\n",
    "\n",
    "        with open(index_path,'r') as f :\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if '\\t' in line:\n",
    "                    #re按照符号切分\n",
    "                    DN_and_TF = re.split('([0-9]+)',line)\n",
    "                    #filter只保留数字\n",
    "                    DN_and_TF = list(filter(str.isdigit, DN_and_TF))\n",
    "                    dn.append(DN_and_TF[0])\n",
    "                    tf.append(len(DN_and_TF)-1)\n",
    "                else:\n",
    "                    word_document_number.append(dn)\n",
    "                    word_tf.append(tf)\n",
    "                    dn = []\n",
    "                    tf = []\n",
    "                    W_and_DF = re.split(r'[:()]',line)\n",
    "                    W_and_DF = list(filter(str.isalnum, W_and_DF))\n",
    "                    word_list.append(W_and_DF[0])\n",
    "                    Document_f.append(W_and_DF[1])\n",
    "        f.close()\n",
    "        \n",
    "        stop_words = []\n",
    "        with open('englishST.txt','r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                stop_words.append(line.strip('\\n'))\n",
    "        f.close()\n",
    "        self.stop_words = stop_words\n",
    "        \n",
    "        word_document_number.pop(0)\n",
    "        word_tf.pop(0)\n",
    "        \n",
    "        self.word_list = word_list\n",
    "        self.Document_f = Document_f\n",
    "        self.word_document_number = word_document_number\n",
    "        self.word_tf = word_tf\n",
    "        \n",
    "    #读书名和作者的index，结构相同    \n",
    "    def ReadTIndex(self, title_index_path):\n",
    "        word_list = []\n",
    "        word_document_number = []\n",
    "        word_docu_position = []\n",
    "        dp = []\n",
    "        dn = []\n",
    "\n",
    "        with open(title_index_path,'r') as f :\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if '\\t' in line:\n",
    "                    #re按照符号切分\n",
    "                    DN_and_TF = re.split('([0-9]+)',line)\n",
    "                    #filter只保留数字\n",
    "                    DN_and_TF = list(filter(str.isdigit, DN_and_TF))\n",
    "                    dn.append(DN_and_TF[0])\n",
    "                    dp.append(DN_and_TF[1:])\n",
    "                else:\n",
    "                    word_docu_position.append(dp)\n",
    "                    word_document_number.append(dn)\n",
    "                    dp = []\n",
    "                    dn = []\n",
    "                    W_and_DF = re.split(r'[:()]',line)\n",
    "                    W_and_DF = list(filter(str.isalnum, W_and_DF))\n",
    "                    word_list.append(W_and_DF[0])\n",
    "        f.close()\n",
    "        word_docu_position.pop(0)\n",
    "        word_document_number.pop(0)\n",
    "        \n",
    "        self.title_list = word_list\n",
    "        self.title_document_number = word_document_number\n",
    "        self.title_docu_position = word_docu_position\n",
    "    \n",
    "    def ReadWIndex(self, writer_index_path):\n",
    "        word_list = []\n",
    "        word_document_number = []\n",
    "        word_docu_position = []\n",
    "        dp = []\n",
    "        dn = []\n",
    "\n",
    "        with open(writer_index_path,'r') as f :\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if '\\t' in line:\n",
    "                    #re按照符号切分\n",
    "                    DN_and_TF = re.split('([0-9]+)',line)\n",
    "                    #filter只保留数字\n",
    "                    DN_and_TF = list(filter(str.isdigit, DN_and_TF))\n",
    "                    dn.append(DN_and_TF[0])\n",
    "                    dp.append(DN_and_TF[1:])\n",
    "                else:\n",
    "                    word_docu_position.append(dp)\n",
    "                    word_document_number.append(dn)\n",
    "                    dp = []\n",
    "                    dn = []\n",
    "                    W_and_DF = re.split(r'[:()]',line)\n",
    "                    W_and_DF = list(filter(str.isalnum, W_and_DF))\n",
    "                    word_list.append(W_and_DF[0])\n",
    "        f.close()\n",
    "        word_docu_position.pop(0)\n",
    "        word_document_number.pop(0)\n",
    "        \n",
    "        self.writer_list = word_list\n",
    "        self.writer_document_number = word_document_number\n",
    "        self.writer_docu_position = word_docu_position\n",
    "    \n",
    "    #读XML获取文件数量，整合代码可以直接使用self里的document进行文件提取\n",
    "    def ReadCSV(self, csv_path):\n",
    "        #这里只是为了获得有多少个文件\n",
    "        with open(csv_path, 'r') as f:\n",
    "            self.document_nub = len(f.readlines())\n",
    "        f.close()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    搜索框启动器\n",
    "    '''\n",
    "    #搜索框启动器\n",
    "    def Start(self):\n",
    "        print('\\nPlease enter the query:')\n",
    "        self.query = input()\n",
    "        #将query去符号并分割\n",
    "        begin_time = time()\n",
    "        falg = self.judgeQueries(self.query)\n",
    "        queryL = self.excuteQueries(self.query)\n",
    "        if flag == 'B':\n",
    "            documents = self.BW_PhraseSearch(queryL,self.title_list,self.title_document_number,self.title_docu_position)\n",
    "        elif flag == 'W':\n",
    "            documents = self.BW_PhraseSearch(queryL,self.writer_list,self.writer_document_number,self.writer_docu_position)\n",
    "        else:\n",
    "            documents = self.RSearch(queryL)\n",
    "        end_time = time()\n",
    "        print(\"Search Type:\",flag)\n",
    "        print(\"搜索结果是：\", documents[:10])\n",
    "        print('Time is:', end_time-begin_time)\n",
    "    \n",
    "    '''\n",
    "    LinearSVM判定query是作者名、书名或混合\n",
    "    在class初始化环节完成模型的训练and save model，搜索框出现直接使用模型\n",
    "    '''\n",
    "    def LinearSVM(self, data_path):\n",
    "        books_list = []\n",
    "        writers_list = []\n",
    "        des_list = []\n",
    "        doc_text = []\n",
    "        lable_list = []\n",
    "        LSVM_word_list = []\n",
    "        punct = re.compile(f'[{string.punctuation}]')\n",
    "        data = pd.read_csv(data_path)\n",
    "    \n",
    "        books_list = list(data['title'])\n",
    "        books_list = [i for i in books_list if i != ' ']\n",
    "        for lines in books_list:\n",
    "            text =  punct.sub('',str(lines)).split()\n",
    "            for word in text:\n",
    "                if len(word)!=0:\n",
    "                    LSVM_word_list.append(word)\n",
    "                else:\n",
    "                    continue\n",
    "            lines = text\n",
    "            doc_text.append(lines)\n",
    "            lable_list.append(\"B\")\n",
    "\n",
    "        writers_list = list(data['author'])\n",
    "        writers_list = [i for i in writers_list if i != ' ']\n",
    "        for lines in writers_list:\n",
    "            text =  punct.sub('',str(lines)).split()\n",
    "            for word in text:\n",
    "                if len(word)!=0:\n",
    "                    LSVM_word_list.append(word)\n",
    "                else:\n",
    "                    continue\n",
    "            lines = text\n",
    "            doc_text.append(lines)\n",
    "            lable_list.append(\"W\")\n",
    "\n",
    "        des_list = list(data['description'])\n",
    "        des_list = [i for i in des_list if i != ' ']\n",
    "        for lines in des_list:\n",
    "            text =  punct.sub('',str(lines)).split()\n",
    "            for word in text:\n",
    "                if len(word)!=0:\n",
    "                    LSVM_word_list.append(word)\n",
    "                else:\n",
    "                    continue\n",
    "            lines = text\n",
    "            doc_text.append(lines)\n",
    "            lable_list.append(\"D\")\n",
    "\n",
    "\n",
    "        LSVM_word_list = set(LSVM_word_list)\n",
    "        wordDic = {}\n",
    "        for word_id, wordc in enumerate(LSVM_word_list):\n",
    "            wordDic[wordc] = word_id\n",
    "\n",
    "        with open(\"LSVM_wordDic\",\"w\") as fi:\n",
    "            for word in wordDic:\n",
    "                fi.write(str(word)+\"\\n\")\n",
    "        fi.close()\n",
    "            \n",
    "        matrix_size = (len(doc_text),len(wordDic)+1)\n",
    "        indeX = len(wordDic)\n",
    "        X = scipy.sparse.dok_matrix(matrix_size)\n",
    "        for doc_id, doc in enumerate(doc_text):\n",
    "            for w in doc:\n",
    "                X[doc_id,wordDic.get(w,indeX)] += 1\n",
    "\n",
    "        train_X, test_X, y_train, y_test = sklearn.model_selection.train_test_split(X, lable_list, test_size=0.2, shuffle = True, stratify=lable_list)\n",
    "        model = svm.LinearSVC(C = 0.1, loss = 'squared_hinge',multi_class='crammer_singer',penalty= 'l2')\n",
    "        model.fit(train_X, y_train)\n",
    "        file = open(\"model.pickle\",\"wb\")\n",
    "        pickle.dump(model,file)\n",
    "        file.close()\n",
    "        prediction_t = model.predict(test_X)\n",
    "        print(\"\\nreport:\")\n",
    "        print(classification_report(y_test,prediction_t,target_names=['B','W','M']))\n",
    "        \n",
    "    '''\n",
    "    下面是精彩的search组件\n",
    "    '''\n",
    "    #query切片\n",
    "    def excuteQueries(self, query):\n",
    "        queryWords = re.split(\" \", query)\n",
    "        query_word_list = []\n",
    "        \n",
    "        for word in queryWords:\n",
    "            word = word.lower()\n",
    "            for w in word:\n",
    "                if w in string.punctuation:\n",
    "                    word = word.replace(w,\"\")\n",
    "            query_word_list.append(word)\n",
    "            \n",
    "        query_word_list = [i for i in query_word_list if i != '']        \n",
    "        return query_word_list\n",
    "    \n",
    "    def ReadLSVM_dic(self):\n",
    "        wordlist = []\n",
    "        with open(\"LSVM_wordDic\",'r') as fi:\n",
    "            lines = fi.readlines()\n",
    "            for line in lines:\n",
    "                wordlist.append(str(line).strip())\n",
    "        fi.close()\n",
    "        wordDic = {}\n",
    "        for word_id, wordc in enumerate(wordlist):\n",
    "            wordDic[wordc] = word_id\n",
    "        self.wordDic = wordDic\n",
    "    \n",
    "    def judgeQueries(self, query):\n",
    "        punct = re.compile(f'[{string.punctuation}]')\n",
    "        text =  punct.sub('',str(query)).split()\n",
    "        wordDic = self.wordDic\n",
    "\n",
    "        matrix_size = (1,len(wordDic)+1)\n",
    "        X = scipy.sparse.dok_matrix(matrix_size)\n",
    "        for w in text:\n",
    "            X[0,wordDic.get(w)] += 1\n",
    "\n",
    "        file = open(\"model.pickle\", \"rb\")\n",
    "        model = pickle.load(file)\n",
    "        file.close()\n",
    "        flag = model.predict(X)\n",
    "        \n",
    "        return flag\n",
    "    \n",
    "    '''\n",
    "    混合Rank搜索_TFIDF\n",
    "    '''\n",
    "    def get_word_rank_score(self, wordR):\n",
    "        #score = (1+lg(tf))*lg(dn/df)\n",
    "        word_list = self.word_list\n",
    "        worddf = self.Document_f\n",
    "        dn = int(self.document_nub)\n",
    "\n",
    "        scoreR = []\n",
    "        log = math.log\n",
    "        Numb = int(word_list.index(wordR))\n",
    "        \n",
    "        s_word_document_number = self.word_document_number[Numb]\n",
    "        Df = float(worddf[Numb])\n",
    "        Tf = self.word_tf[Numb]\n",
    "        for i in range(0, len(s_word_document_number)):   \n",
    "            Score = (1+log(int(Tf[i]),10))*log((dn/Df),10)\n",
    "            Score = float(format(Score, '.4f'))\n",
    "            scoreR.append(Score)\n",
    "        return s_word_document_number,scoreR\n",
    "    \n",
    "    \n",
    "    def RSearch(self, queryL):\n",
    "        Wdocument_number = []\n",
    "        WScore = []\n",
    "        Dic_words_score = {}\n",
    "        Rdocument = []\n",
    "        Rscore = []\n",
    "        query_word_list = []\n",
    "        BDocuments_List = []\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        stop_words = self.stop_words\n",
    "        word_list = self.word_list\n",
    "        Word_Documentno = self.word_document_number\n",
    "        \n",
    "        for word in queryL:\n",
    "            if word not in stop_words:\n",
    "                word = porter_stemmer.stem(word)\n",
    "                query_word_list.append(word)\n",
    "        queryL = query_word_list\n",
    "  \n",
    "        BDocuments_List = Word_Documentno[word_list.index(queryL[0])]\n",
    "        for words in range(1, len(queryL)):\n",
    "            tempL = Word_Documentno[word_list.index(queryL[words])]\n",
    "            BDocuments_List = list(set(BDocuments_List).intersection(set(tempL)))\n",
    "        if len(BDocuments_List)==1:\n",
    "            Rdocument = BDocuments_List\n",
    "        else:\n",
    "            for word in queryL:\n",
    "                Wdocument_number, WScore = self.get_word_rank_score(word)\n",
    "                a = len(Wdocument_number)\n",
    "                for y in range(0,a):\n",
    "                    if Wdocument_number[y] not in Dic_words_score:\n",
    "                        Dic_words_score[Wdocument_number[y]] = float(WScore[y])\n",
    "                    else:\n",
    "                        Dic_words_score[Wdocument_number[y]] = float(Dic_words_score[Wdocument_number[y]])+float(WScore[y])\n",
    "    \n",
    "            Dic_words_score = sorted(Dic_words_score.items(), key=lambda d: d[1], reverse=True)\n",
    "            for term in Dic_words_score:\n",
    "                Rdocument.append(term[0])\n",
    "        \n",
    "        return Rdocument\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    书名作者名的phrase\n",
    "    '''\n",
    "    def BW_PhraseSearch(self, queryL,word_list,Word_Documentno,word_position):\n",
    "        documentS = []\n",
    "        CHA = {}\n",
    "    \n",
    "        #以第一个单词的documentL为基准点，取交集\n",
    "        BDocuments_List = Word_Documentno[word_list.index(queryL[0])]\n",
    "        for words in range(1, len(queryL)):\n",
    "            tempL = Word_Documentno[word_list.index(queryL[words])]\n",
    "            BDocuments_List = list(set(BDocuments_List).intersection(set(tempL)))\n",
    "    \n",
    "        #如果没有交集，取并集\n",
    "        if len(BDocuments_List)==0:\n",
    "            BDocuments_List = Word_Documentno[word_list.index(queryL[0])]\n",
    "            for words in range(1, len(queryL)):\n",
    "                tempL = Word_Documentno[word_list.index(queryL[words])]\n",
    "                BDocuments_List = list(set(BDocuments_List).union(set(tempL)))\n",
    "            documentS = BDocuments_List\n",
    "        #如果有唯一交集，直接输出\n",
    "        elif len(BDocuments_List)==1:\n",
    "            documentS = BDocuments_List\n",
    "        #如果有交集且不唯一，计算区间，优先近的文件\n",
    "        else:\n",
    "            print(BDocuments_List)\n",
    "            for i in range(0,len(queryL)-1):\n",
    "                posiL1 = word_position[word_list.index(queryL[i])]\n",
    "                posiL2 = word_position[word_list.index(queryL[i+1])]\n",
    "                donuL1 = Word_Documentno[word_list.index(queryL[i])]\n",
    "                donuL2 = Word_Documentno[word_list.index(queryL[i+1])]\n",
    "                for docu in BDocuments_List:\n",
    "                    l1 = posiL1[donuL1.index(docu)]\n",
    "                    l2 = posiL2[donuL2.index(docu)]\n",
    "                    cha = []\n",
    "                    for x in l1:\n",
    "                        for y in l2:\n",
    "                            z = abs(int(x)-int(y))\n",
    "                            cha.append(z)\n",
    "                    if 1 in cha:\n",
    "                        documentS.append(docu)\n",
    "                    else:\n",
    "                        if str(docu) not in CHA:\n",
    "                            CHA[str(docu)] = sum(cha)\n",
    "                        else:\n",
    "                            CHA[str(docu)] = CHA[str(docu)] + sum(cha)\n",
    "\n",
    "        CHA = sorted(CHA.items(), key=lambda d: d[1], reverse=True)\n",
    "        for term in CHA:\n",
    "            documentS.append(term[0])\n",
    "        documentS = list(set(documentS))\n",
    "\n",
    "        return documentS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the query:\n",
      "Between Two Fires: American Indians in the Civil War\n",
      "Search Type: ['B']\n",
      "搜索结果是： ['1']\n",
      "Time is: 0.028199434280395508\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    index_path = \"content.txt\"\n",
    "    csv_path = \"GoodReads_processed5.csv\"\n",
    "    title_index_path = \"title.txt\"\n",
    "    writer_index_path = \"author.txt\"\n",
    "    Se = Search(index_path,title_index_path,writer_index_path ,csv_path)\n",
    "    Se.Start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.97      0.90      0.93     19267\n",
      "           W       1.00      0.98      0.99     19268\n",
      "           M       0.91      0.99      0.95     19268\n",
      "\n",
      "    accuracy                           0.96     57803\n",
      "   macro avg       0.96      0.96      0.96     57803\n",
      "weighted avg       0.96      0.96      0.96     57803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LSVM training. Save model and wordList for query judgement.\n",
    "if __name__ == '__main__':\n",
    "    #CW1的index\n",
    "    index_path = \"content.txt\"\n",
    "    csv_path = \"GoodReads_processed5.csv\"\n",
    "    title_index_path = \"title.txt\"\n",
    "    writer_index_path = \"author.txt\"\n",
    "    #输入CW1xml的headline和文件内容可以查询到得分前十的文件id.\n",
    "    Se = Search(index_path,title_index_path,writer_index_path ,csv_path)\n",
    "    Se.LinearSVM(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 条件搜索\n",
    "\n",
    "输入条件单词，搜索结果取两个单词的和（Boolean）\n",
    "\n",
    "界面可以做成方框打钩（或者做的有创意点，摇一摇什么的，随机生成几个词条，然后摇一摇再随机生成之类花里胡哨的玩意），一共有30个种类，存放在range_list中。\n",
    "\n",
    "注意，这里输入的query为：A,B,C   \n",
    "\n",
    "的形式。前端可以将界面中选择的词条按这个形式输入，或者更改exQueries，最终queryL的形式是list：['A','B','C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RangeSearch():\n",
    "    def __init__(self,index_path):\n",
    "        self.range_list = []\n",
    "        self.documents_list = []\n",
    "        self.ReadIndex(index_path)\n",
    "    \n",
    "    def Start(self):\n",
    "        print('\\nPlease enter the query:')\n",
    "        self.query = input()\n",
    "        begin_time = time()\n",
    "        queryL = self.exQueries(self.query)\n",
    "        print(queryL)\n",
    "        documents = self.BooleanAndSearch(queryL)\n",
    "        end_time = time()\n",
    "        print(\"搜索结果是：\", documents)\n",
    "        print('Time is:', end_time-begin_time)\n",
    "    \n",
    "    \n",
    "    def ReadIndex(self, index_path):\n",
    "        range_list = []\n",
    "        documents_list = []\n",
    "        \n",
    "        with open(index_path,'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                ss = re.split(r':',line)\n",
    "                temp = re.split('([0-9]+)',ss[1])\n",
    "                temp = list(filter(str.isdigit, temp))\n",
    "                ss[1] = list(temp)\n",
    "                range_list.append(ss[0])\n",
    "                documents_list.append(ss[1])\n",
    "        file.close()\n",
    "        self.range_list = range_list\n",
    "        self.documents_list = documents_list\n",
    "        \n",
    "    def exQueries(self, query):\n",
    "        temp = re.split(r',',query)\n",
    "        queryL = list(temp)\n",
    "        return queryL\n",
    "    \n",
    "    def BooleanAndSearch(self,queryL):\n",
    "        range_list = self.range_list\n",
    "        documents_list = self.documents_list\n",
    "        docuS = documents_list[range_list.index(queryL[0])]\n",
    "        for i in range(1,len(queryL)):\n",
    "            tempL = documents_list[range_list.index(queryL[i])]\n",
    "            #这里取得是交集\n",
    "            docuS = list(set(docuS).intersection(set(tempL)))\n",
    "        \n",
    "        #如果选了三个以上的标签会出现没那种书的情况，所以每两种取交集再取并集\n",
    "        if len(docuS)==0:\n",
    "            a = 2\n",
    "            while (a < len(queryL)):\n",
    "                tempL1 = documents_list[range_list.index(queryL[a])]\n",
    "                tempL2 = documents_list[range_list.index(queryL[a-1])]\n",
    "                tempL3 = list(set(tempL1).intersection(set(tempL2)))\n",
    "                a += 1\n",
    "                docuS = list(set(tempL3).union(set(docuS)))\n",
    "        \n",
    "        return docuS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    index_path = \"genre.txt\"\n",
    "    Se = RangeSearch(index_path)\n",
    "    Se.Start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter the query:\n",
    "\n",
    "Nonfiction,Classics,Sequential Art,Childrens\n",
    "['Nonfiction', 'Classics', 'Sequential Art', 'Childrens']\n",
    "\n",
    "搜索结果是： ['79118', '50049', '80377', '32338', '86708', '41465', '46247', '32134', '61221', '49387', '94992', '4993', '32177', '50968', '50985', '66414', '93451', '28301', '17690', '58802', '87047', '4012', '17695', '3187', '20915', '75287', '95614', '74462', '32164', '81225', '81786', '39320', '21134', '32176', '17613', '6360', '32324', '52025', '28006', '20913', '17676', '4750']\n",
    "\n",
    "Time is: 0.003989696502685547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matsumura\n",
      "1\n",
      "['96330']\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#ReadIndex 的BUG\n",
    "word_list = []\n",
    "Document_f = []\n",
    "word_document_number = []\n",
    "word_tf = []\n",
    "dn = []\n",
    "tf = []\n",
    "\n",
    "with open(\"author.txt\",'r') as f :\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if '\\t' not in line:\n",
    "            W_and_DF = re.split(r'[:()]',line)\n",
    "            W_and_DF = list(filter(str.isalnum, W_and_DF))\n",
    "            word_list.append(W_and_DF[0])\n",
    "            Document_f.append(W_and_DF[1])\n",
    "            word_document_number.append(dn)\n",
    "            word_tf.append(tf)\n",
    "            dn = []\n",
    "            tf = []\n",
    "        else:\n",
    "            #re按照符号切分\n",
    "            DN_and_TF = re.split('([0-9]+)',line)\n",
    "            #filter只保留数字\n",
    "            DN_and_TF = list(filter(str.isdigit, DN_and_TF))\n",
    "            dn.append(DN_and_TF[0])\n",
    "            tf.append(len(DN_and_TF)-1)\n",
    "f.close()\n",
    "\n",
    "word_document_number.pop(0)\n",
    "word_tf.pop(0)\n",
    "\n",
    "#有个bug在这，最后一行没有读，所以后两个表比前两个表少一个值。。。我一直看不出来ORZ\n",
    "print(word_list[-1])\n",
    "print(Document_f[-1])\n",
    "print(word_document_number[-1])\n",
    "print(word_tf[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
